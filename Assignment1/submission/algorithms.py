'''
Author: Manas Vashistha
'''
import numpy as np

class Algorithms:
    def __init__(self, num_arms):
        '''
        Parent class for all the algorithms.

        num_arms: Number of arms of the bandit.
        '''
        self.num_arms = num_arms
        self.arm_pulls = np.zeros(self.num_arms, dtype=int) # pulls of each arm (updated after each pull)
        self.pulls = 0 # total pulls for a horizon
        self.cum_rew = 0 # total reward for a horizon
        self.rewards = np.zeros(self.num_arms) # reward for each arm (updated after each pull)

    def execute(self, instance):
        '''
        The main function where the arm to be pulled is sampled using a particular algorithm.
        '''
        self.at = self.run_algo() # arm to be pulled sampled by the function run_algo
        self.rt = instance.pull_arm(self.at) # reward generated by pulling the arm at
        self.arm_pulls[self.at] += 1 # increment pull of arm at
        self.rewards[self.at] += self.rt # add the reward to rewards of arm at
        self.pulls = np.sum(self.arm_pulls) # calculate total pulls till each step
        self.cum_rew = np.sum(self.rewards) # calculate total reward till each step
    
    def run_algo(self):
        '''
        Implemented below.
        '''
        raise NotImplementedError
    
    def tie_breaker(self, x):
        '''
        If there are more than one max value select the argmax randomly.
        '''
        return np.random.choice(np.where(x == x.max())[0])


class EpsilonGreedy(Algorithms):
    def __init__(self, epsilon, num_arms):
        '''
        Epsilon Greedy
        '''
        super().__init__(num_arms)
        self.epsilon = epsilon

    def run_algo(self):
        '''
        Select a new arm randomly with epsilon probability and explores with (1-epsilon) probability.
        '''
        if np.random.binomial(1, self.epsilon):
            return np.random.randint(self.num_arms) # randomly select an arm
        
        pt = np.zeros_like(self.rewards) # initialise all p_hats to 0
        idxs = (self.arm_pulls != 0)
        pt[idxs] = self.rewards[idxs] / self.arm_pulls[idxs] # calculate p_hat where arm_pulls is not 0
        return self.tie_breaker(pt) # break ties
        
class UCB(Algorithms):
    def __init__(self, num_arms):
        '''
        UCB (Upper Confidence Bound)
        '''
        super().__init__(num_arms)
    
    def run_algo(self):
        '''
        1. Pull arms in order till all the arms are pulled exactly once.
        2. After that pull the arm for which UCB is maximum.
        '''
        if self.pulls < self.num_arms:
            return self.pulls # sample all the arms exactly once while starting
        pt = self.rewards / self.arm_pulls
        ucbt = pt + np.sqrt(2 * np.log(self.pulls) / self.arm_pulls) # calculate ucb for all arms
        return self.tie_breaker(ucbt) # break ties

class KLUCB(Algorithms):
    def __init__(self, num_arms):
        '''
        KL-UCB (KL-Upper Confidence Bound)
        '''
        super().__init__(num_arms)
        self.c = 3
    
    def run_algo(self):
        '''
        1. Pull arms in order till all the arms are pulled exactly once.
        2. After that pull the arm for which UCB is maximum.
        '''
        if self.pulls < self.num_arms:
            return self.pulls # sample all the arms exactly once while starting
            
        def kld(x, y):
            '''
            Calculate KL-Divergence of given.
            x: an array of floats
            y: an array of floats

            return x*log(x/y) + (1-x)*log((1-x)/(1-y))
            '''
            return x * np.log(x / y) + (1 - x) * np.log((1 - x) / (1 - y))
        
        def bin_search(left, right, p, bound, threshold=1e-4):
            '''
            Binary search for finding optimal q in the range [p_hat, 1]
            '''
            mid = (left + right) / 2 # q1 = (p_hat + 1)/2
            mid_kl = kld(p, mid) # calculate KL-UCB of p_hat and q
            limit = mid_kl - bound # the objective function to be constrained
            while not ((np.abs(limit) < threshold).all() and (np.abs(left - right) < threshold).all()):
                '''
                Update rules-
                For a q1 if KL(p, q1) - Bound <= 0 search in [q1, 1] (maximise q) else in [p_hat, q1] (find the q to satisfy condition).
                '''
                left, right = np.where((limit <= 0), mid, left), np.where((limit <= 0), right, mid) 
                mid = (left + right) / 2
                mid_kl = kld(p, mid)
                limit = mid_kl - bound
            return mid

        pt = self.rewards / self.arm_pulls  

        if (pt == 1).any(): 
            return self.tie_breaker(pt) # chose randomly one p_hat which satisfies p_hat ==1
        
        z_idxs = (pt == 0)
        nz_idxs = (pt != 0)
        q_opt = np.zeros_like(pt)
        bound = (np.log(self.pulls) + self.c * np.log(np.log(self.pulls))) / self.arm_pulls
        q_opt[z_idxs] = 1 - np.exp(-bound[z_idxs]) # explicit form of the constraint when p_hat = 0
        if (nz_idxs).any():
            q_opt[nz_idxs] = bin_search(pt[nz_idxs], np.ones_like(q_opt[nz_idxs]), pt[nz_idxs], bound[nz_idxs]) # binary search
        return self.tie_breaker(q_opt) 

class Thompson(Algorithms):
    def __init__(self, num_arms):
        '''
        Thompson Sampling
        '''
        super().__init__(num_arms)
    
    def run_algo(self):
        return self.tie_breaker(np.random.beta(self.rewards + 1, self.arm_pulls - self.rewards + 1))

class ThompsonHint(Algorithms):
    def __init__(self, num_arms, hint):
        '''
        Thompson Sampling With Hint
        hint: a permutation of arm means
        '''
        super().__init__(num_arms)
        self.hint = hint
        self.p_arg = np.argmax(self.hint) # argument of maximum mean in hint
        # print(self.p_arg)
        self.mypdf = np.zeros((self.num_arms, self.num_arms)) # initialise and array with 1/num_arms to generate pdf of hints
        self.mypdf.fill(1 / self.num_arms)
    
    def run_algo(self):
        '''
        1. First pull return arm using Thompson Sampling.
        2. See previous reward (rt) and arm (at), if 1 multiply the pdf of hint for at with hint else multiply it with (1-hint).
        3. Normalize the pdf of hint for at
        4. return the arm which has maximum value for p_arg column. If more than one break ties randomly
        '''
        if self.pulls == 0:
            return self.tie_breaker(np.random.beta(self.rewards + 1, self.arm_pulls - self.rewards + 1))
        
        if self.rt == 1:
            self.mypdf[self.at] *= self.hint
        else:
            self.mypdf[self.at] *= (1 - self.hint)
        
        self.mypdf[self.at] = self.mypdf[self.at] / np.sum(self.mypdf[self.at])
        pos = self.mypdf[:, self.p_arg]       
        return self.tie_breaker(pos)